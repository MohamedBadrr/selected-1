{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1010/1010 [00:04<00:00, 235.28it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1010/1010 [00:04<00:00, 250.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1010/1010 [00:04<00:00, 237.33it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1016/1016 [00:04<00:00, 222.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1010/1010 [00:04<00:00, 234.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1010/1010 [00:04<00:00, 239.11it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1009/1009 [00:04<00:00, 232.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1008/1008 [00:04<00:00, 242.81it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1010/1010 [00:04<00:00, 222.99it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1010/1010 [00:04<00:00, 246.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1008/1008 [00:03<00:00, 260.79it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1008/1008 [00:04<00:00, 245.66it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1008/1008 [00:04<00:00, 243.30it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1008/1008 [00:05<00:00, 197.02it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1008/1008 [00:04<00:00, 239.43it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1007/1007 [00:04<00:00, 236.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1011/1011 [00:03<00:00, 255.50it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATADIR = \"E:\\\\UNIVERSIY\\\\PROJECTS\\\\LEVEL_3\\\\First_Term\\\\SeLected\\\\dataset\\\\English\\\\Fnt\\\\train\"\n",
    "# DATADIR = \"E:\\ProjectSelected\\English\\Fnt\"\n",
    "\n",
    "CATEGORIES = [\"Sample001\",\"Sample002\",\"Sample003\",\"Sample004\",\"Sample005\",\"Sample006\",\n",
    "              \"Sample007\",\"Sample008\",\"Sample009\",\"Sample010\",\"Sample019\",\"Sample020\",\"Sample021\",\"Sample022\"\n",
    "             ,\"Sample023\",\"Sample024\",'Sample042']\n",
    "\n",
    "training_data = []\n",
    "IMG_SIZE = 75\n",
    "\n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:  \n",
    "\n",
    "        path = os.path.join(DATADIR, category)  # create path to classes\n",
    "        class_num = CATEGORIES.index(category)  # get the classification\n",
    "        # tqdm : عشان تظهر الخط الاحمر دا ف تحميل الداتا اشوفها وهيبتحمل \n",
    "        for img in tqdm(os.listdir(path)):  # iterate over each image per class\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
    "                training_data.append([new_array, class_num])  # add this to our training_data\n",
    "            # except Exception as e:  # in the interest in keeping the output clean...\n",
    "            #     pass\n",
    "            except Exeption as e:\n",
    "                pass\n",
    "\n",
    "\n",
    "create_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17161\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randome the dataset for good training\n",
    "random.shuffle(training_data)\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "#build input (x) , output (y) arrays\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "X=np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "#data normalization\n",
    "X_valid, X_train = X_train_full[:2700] / 255., X_train_full[2700:]/255.\n",
    "y_valid, y_train = y_train_full[:2700], y_train_full[2700:]\n",
    "X_test = X_test / 255.\n",
    "\n",
    "X_train= np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_valid = np.asarray(X_valid)\n",
    "y_valid = np.asarray(y_valid)\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "#save the dataset so that we don't need to keep calculating it every time we want\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"0\" : 0,\"1\" : 1 ,\"2\" :2 ,\"3\" : 3,\"4\" : 4,\"5\" : 5,\n",
    "              \"6\" : 6,\"7\" : 7,\"8\" : 8,\"9\" :9,\"i\" : 10, \"j\" : 11,\"k\" : 12,\"L\":13\n",
    "             ,\"M\" : 14,\"N\":15 , 'F':16 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getclass (n):\n",
    "    for x,y in data.items():\n",
    "         if n==y :\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n"
     ]
    }
   ],
   "source": [
    "print(getclass(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data in pickle\n",
    "import pickle\n",
    "pickle_out=open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out=open(\"y.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "345/345 [==============================] - 19s 55ms/step - loss: 1.2209 - accuracy: 0.6568 - val_loss: 0.6471 - val_accuracy: 0.8181\n",
      "Epoch 2/20\n",
      "345/345 [==============================] - 20s 57ms/step - loss: 0.5520 - accuracy: 0.8377 - val_loss: 0.3973 - val_accuracy: 0.8822\n",
      "Epoch 3/20\n",
      "345/345 [==============================] - 18s 53ms/step - loss: 0.4212 - accuracy: 0.8754 - val_loss: 0.3594 - val_accuracy: 0.9011\n",
      "Epoch 4/20\n",
      "345/345 [==============================] - 20s 58ms/step - loss: 0.3676 - accuracy: 0.8949 - val_loss: 0.3783 - val_accuracy: 0.8919\n",
      "Epoch 5/20\n",
      "345/345 [==============================] - 19s 56ms/step - loss: 0.3363 - accuracy: 0.8975 - val_loss: 0.2865 - val_accuracy: 0.9178\n",
      "Epoch 6/20\n",
      "345/345 [==============================] - 19s 56ms/step - loss: 0.2950 - accuracy: 0.9086 - val_loss: 0.3549 - val_accuracy: 0.8911\n",
      "Epoch 7/20\n",
      "345/345 [==============================] - 21s 60ms/step - loss: 0.2880 - accuracy: 0.9107 - val_loss: 0.2713 - val_accuracy: 0.9159\n",
      "Epoch 8/20\n",
      "345/345 [==============================] - 19s 54ms/step - loss: 0.2676 - accuracy: 0.9194 - val_loss: 0.2919 - val_accuracy: 0.9096\n",
      "Epoch 9/20\n",
      "345/345 [==============================] - 19s 55ms/step - loss: 0.2344 - accuracy: 0.9263 - val_loss: 0.2345 - val_accuracy: 0.9230\n",
      "Epoch 10/20\n",
      "345/345 [==============================] - 19s 56ms/step - loss: 0.2204 - accuracy: 0.9294 - val_loss: 0.2994 - val_accuracy: 0.8959\n",
      "Epoch 11/20\n",
      "345/345 [==============================] - 19s 55ms/step - loss: 0.2357 - accuracy: 0.9248 - val_loss: 0.2712 - val_accuracy: 0.9215\n",
      "Epoch 12/20\n",
      "345/345 [==============================] - 20s 59ms/step - loss: 0.1923 - accuracy: 0.9365 - val_loss: 0.2168 - val_accuracy: 0.9330\n",
      "Epoch 13/20\n",
      "345/345 [==============================] - 20s 57ms/step - loss: 0.1744 - accuracy: 0.9428 - val_loss: 0.2237 - val_accuracy: 0.9352\n",
      "Epoch 14/20\n",
      "345/345 [==============================] - 17s 50ms/step - loss: 0.1890 - accuracy: 0.9405 - val_loss: 0.2135 - val_accuracy: 0.9385\n",
      "Epoch 15/20\n",
      "345/345 [==============================] - 17s 48ms/step - loss: 0.1640 - accuracy: 0.9462 - val_loss: 0.2379 - val_accuracy: 0.9274\n",
      "Epoch 16/20\n",
      "345/345 [==============================] - 20s 57ms/step - loss: 0.1858 - accuracy: 0.9380 - val_loss: 0.2280 - val_accuracy: 0.9285\n",
      "Epoch 17/20\n",
      "345/345 [==============================] - 19s 54ms/step - loss: 0.1602 - accuracy: 0.9467 - val_loss: 0.2141 - val_accuracy: 0.9352\n",
      "Epoch 18/20\n",
      "345/345 [==============================] - 18s 53ms/step - loss: 0.1468 - accuracy: 0.9528 - val_loss: 0.2782 - val_accuracy: 0.9185\n",
      "Epoch 19/20\n",
      "345/345 [==============================] - 20s 57ms/step - loss: 0.1478 - accuracy: 0.9518 - val_loss: 0.2674 - val_accuracy: 0.9170\n",
      "Epoch 20/20\n",
      "345/345 [==============================] - 19s 54ms/step - loss: 0.1552 - accuracy: 0.9499 - val_loss: 0.2801 - val_accuracy: 0.9156\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.2975 - accuracy: 0.9155\n",
      "loss is :  0.2974962890148163\n",
      "accuracy is :  0.9155257940292358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Image_ANN.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Image_ANN.model\\assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Dropout , Activation , Flatten , Conv2D , MaxPooling2D\n",
    "# ANN model\n",
    "model = tf.keras.models.Sequential()\n",
    "# we will add some layers وبنبدء بالفلاترن لير لانها تعتبر الباداية ي معلم و بندخل هنا ال input shape \n",
    "# input_shape=(IMG_SIZE,IMG_SIZE)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(17, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=\"adam\",\n",
    "             metrics=['accuracy'],\n",
    "             run_eagerly=True)\n",
    "\n",
    "history = model.fit(X_train,y_train,epochs=20,validation_data=(X_valid,y_valid))\n",
    "\n",
    "loss , acc = model.evaluate(X_test,y_test)\n",
    "\n",
    "print('loss is : ',loss)\n",
    "print('accuracy is : ',acc)\n",
    " \n",
    "model.save('Image_ANN.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Selected ANN image.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Selected ANN image.model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('Selected ANN image.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "Prediction is  F\n"
     ]
    }
   ],
   "source": [
    "img_prd = []\n",
    "img_prd = cv2.imread(\"E:\\\\UNIVERSIY\\\\PROJECTS\\\\LEVEL_3\\\\First_Term\\\\SeLected\\dataset\\\\English\\\\test\\\\img042-00018.png\",cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.resize(img_prd,(IMG_SIZE,IMG_SIZE))\n",
    "plt.imshow(img_prd)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "img_prd = np.array([img])\n",
    "img_prd = img_prd/255.\n",
    "result_class = model.predict(img_prd)\n",
    "prediction = np.argmax(result_class)\n",
    "print('Prediction is ',getclass(prediction))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
